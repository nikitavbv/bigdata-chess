- [done] tokenization: https://docs.rs/tokenizers/latest/tokenizers/
  - read from `hn-comments`
  - write to `hn-comments-tokenized`
- [done] lemmatization: using python nltk
  - read from `hn-comments-tokenized`
  - write to `hn-comments-lemmatized`
- [done] sentiment: https://docs.rs/rust-bert/latest/rust_bert/
  - read from `hn-comments`
  - write to `hn-comments-sentiment`
- categories: cluster rust-bert sentence embeddings using kmeans
  - [done] write embeddings
    - read from `hn-comments`
    - write comments to `hn-comments-embeddings`.
  - [done] save from `hn-comments-embeddings` to json file.
  - read json file and cluster using https://docs.rs/linfa-clustering/latest/linfa_clustering/struct.KMeans.html
- [done] hashing (for tfidf): https://docs.rs/murmur3/latest/murmur3/fn.murmur3_32.html
  - read from `hn-comments-tokenized`
  - write to `hn-comments-hashed`
- [done] tfidf: count number of documents with term in them (i.e. term is counted once per document)
  - [done] stats collector:
    - read from `hn-comments-hashed`
    - write stats to leveldb
  - [done] metric calculator:
    - read from `hn-comments-hashed`
    - write metric to `hn-comments-tfidf`
- bag of words write words to csv and analyze using spark
  - read from `hn-comments-hashed`
  - write stats to json
  - implement reading resulting file